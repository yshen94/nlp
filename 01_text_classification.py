# -*- coding: utf-8 -*-
"""01_text-classification.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/huggingface/blog/blob/master/notebooks/trainer/01_text_classification.ipynb

# Trainer: GLUE MNLI example, the Colab version ğŸ”¥

Install `transformers` from master, and also clone the repo to get some utility files
"""

!pip install git+https://github.com/huggingface/transformers

!git clone https://github.com/huggingface/transformers
!python transformers/utils/download_glue_data.py

"""Check that we have a GPU and check its memory size (depending on its RAM size you can change the batch sizes below)"""

!nvidia-smi

"""### All imports are here:"""

import dataclasses
import logging
import os
import sys
from dataclasses import dataclass, field
from typing import Dict, Optional

import numpy as np

from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer, EvalPrediction, GlueDataset
from transformers import GlueDataTrainingArguments as DataTrainingArguments
from transformers import (
    HfArgumentParser,
    Trainer,
    TrainingArguments,
    glue_compute_metrics,
    glue_output_modes,
    glue_tasks_num_labels,
    set_seed,
)

logging.basicConfig(level=logging.INFO)

"""### We use dataclass-based configuration objects, let's define the one related to which model we are going to train here:"""

@dataclass
class ModelArguments:
    """
    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.
    """

    model_name_or_path: str = field(
        metadata={"help": "Path to pretrained model or model identifier from huggingface.co/models"}
    )
    config_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained config name or path if not the same as model_name"}
    )
    tokenizer_name: Optional[str] = field(
        default=None, metadata={"help": "Pretrained tokenizer name or path if not the same as model_name"}
    )
    cache_dir: Optional[str] = field(
        default=None, metadata={"help": "Where do you want to store the pretrained models downloaded from s3"}
    )

"""### Here are all the training parameters we are going to use:"""

model_args = ModelArguments(
    model_name_or_path="distilbert-base-cased",
)
data_args = DataTrainingArguments(task_name="mnli", data_dir="./glue_data/MNLI")
training_args = TrainingArguments(
    output_dir="./models/model_name",
    overwrite_output_dir=True,
    do_train=True,
    do_eval=True,
    per_gpu_train_batch_size=32,
    per_gpu_eval_batch_size=128,
    num_train_epochs=1,
    logging_steps=500,
    logging_first_step=True,
    save_steps=1000,
    evaluate_during_training=True,
)

set_seed(training_args.seed)

"""We fine-tune on MNLI so let's find out the number of labels:"""

num_labels = glue_tasks_num_labels[data_args.task_name]
num_labels

"""## ğŸ¤— Now we can instantiate our config, our tokenizer, and our model"""

config = AutoConfig.from_pretrained(
    model_args.model_name_or_path,
    num_labels=num_labels,
    finetuning_task=data_args.task_name,
)
tokenizer = AutoTokenizer.from_pretrained(
    model_args.model_name_or_path,
)
model = AutoModelForSequenceClassification.from_pretrained(
    model_args.model_name_or_path,
    config=config,
)

# Get datasets
train_dataset = GlueDataset(data_args, tokenizer=tokenizer, limit_length=100_000)
eval_dataset = GlueDataset(data_args, tokenizer=tokenizer, mode='dev')

"""### We need to define a task-specific way of computing relevant metrics (see more details in the Trainer class):"""

def compute_metrics(p: EvalPrediction) -> Dict:
    preds = np.argmax(p.predictions, axis=1)
    return glue_compute_metrics(data_args.task_name, preds, p.label_ids)

"""##### We are now ready to initialize our Trainer"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    compute_metrics=compute_metrics,
)

"""## Launching the training is as simple is doing trainer.train() â™¥ï¸"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# trainer.train()

"""## Check that our training was successful using TensorBoard"""

# Commented out IPython magic to ensure Python compatibility.
# Load the TensorBoard notebook extension
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir runs

"""# ğŸ‰ Yeah it's training!"""

